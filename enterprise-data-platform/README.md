# Enterprise Data Platform

> Production-grade data lakehouse powering supply chain operations at scale

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![Polars](https://img.shields.io/badge/Polars-Latest-orange.svg)](https://pola.rs)
[![DuckDB](https://img.shields.io/badge/DuckDB-Latest-yellow.svg)](https://duckdb.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ¯ Overview

A **Medallion architecture data lakehouse** designed for enterprise retail supply chain operations. This platform processes data from multiple source systems (ERP, CRM, WMS, OBI) and delivers analytics-ready datasets for demand forecasting, inventory optimization, and operational reporting.

### Key Metrics

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| ETL Processing Time | 4 hours | 30 minutes | **87%** â¬‡ï¸ |
| Data Freshness | 48 hours | 2-4 hours | **90%** â¬‡ï¸ |
| Pipeline Reliability | ~70% | 95%+ | **25%** â¬†ï¸ |
| Data Quality Issues | Unknown | 500+ caught | **Automated** |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DATA SOURCES                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     ERP     â”‚     CRM     â”‚     WMS     â”‚     OBI     â”‚   External APIs     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â”‚             â”‚             â”‚                 â”‚
       â–¼             â–¼             â–¼             â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        EXTRACTION LAYER                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  DB Conn  â”‚  â”‚  API      â”‚  â”‚  RPA Bot  â”‚  â”‚  File     â”‚                 â”‚
â”‚  â”‚  Extractorâ”‚  â”‚  Extractorâ”‚  â”‚  Extractorâ”‚  â”‚  Extractorâ”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ğŸ¥‰ BRONZE LAYER                                      â”‚
â”‚                                                                              â”‚
â”‚  â€¢ Raw data landing zone                                                    â”‚
â”‚  â€¢ Immutable historical record                                              â”‚
â”‚  â€¢ Hive-partitioned Parquet (by extract_date)                              â”‚
â”‚  â€¢ Full audit trails                                                        â”‚
â”‚                                                                              â”‚
â”‚  ğŸ“ /lakehouse/bronze/{source}/{table}/extract_date={YYYY-MM-DD}/          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ğŸ¥ˆ SILVER LAYER                                      â”‚
â”‚                                                                              â”‚
â”‚  â€¢ Cleaned & validated data                                                 â”‚
â”‚  â€¢ Schema enforcement                                                       â”‚
â”‚  â€¢ Deduplication & null handling                                           â”‚
â”‚  â€¢ 3-tier anomaly detection                                                â”‚
â”‚                                                                              â”‚
â”‚  ğŸ“ /lakehouse/silver/{domain}/{entity}/                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ğŸ¥‡ GOLD LAYER                                        â”‚
â”‚                                                                              â”‚
â”‚  â€¢ Business-ready star schema                                               â”‚
â”‚  â€¢ 15+ fact tables, 6+ dimension tables                                    â”‚
â”‚  â€¢ Optimized for analytical queries                                         â”‚
â”‚  â€¢ Powers ML models & dashboards                                           â”‚
â”‚                                                                              â”‚
â”‚  ğŸ“ /lakehouse/gold/facts/ & /lakehouse/gold/dimensions/                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CONSUMPTION LAYER                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Streamlit     â”‚   Power BI      â”‚   ML Models     â”‚   Ad-hoc Analysis     â”‚
â”‚   Dashboards    â”‚   Reports       â”‚   (Forecasting) â”‚   (DuckDB)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
enterprise-data-platform/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ medallion-architecture.md    # Detailed layer specifications
â”‚   â”œâ”€â”€ data-flow.md                 # End-to-end data flow documentation
â”‚   â””â”€â”€ system-integration-diagram.png
â”œâ”€â”€ etl-framework/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ pipeline_config.yaml     # Pipeline definitions
â”‚   â”‚   â””â”€â”€ source_config.yaml       # Source system configs
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ base_extractor.py        # Abstract base class
â”‚   â”‚   â”œâ”€â”€ database_extractor.py    # SQL Server, Oracle
â”‚   â”‚   â”œâ”€â”€ api_extractor.py         # REST APIs
â”‚   â”‚   â””â”€â”€ rpa_extractor.py         # Selenium/PyAutoGUI bots
â”‚   â”œâ”€â”€ transformers/
â”‚   â”‚   â”œâ”€â”€ base_transformer.py      # Abstract base class
â”‚   â”‚   â”œâ”€â”€ cleaner.py               # Data cleaning operations
â”‚   â”‚   â”œâ”€â”€ validator.py             # Schema validation
â”‚   â”‚   â””â”€â”€ enricher.py              # Data enrichment
â”‚   â””â”€â”€ loaders/
â”‚       â”œâ”€â”€ base_loader.py           # Abstract base class
â”‚       â””â”€â”€ parquet_loader.py        # Hive-partitioned Parquet
â”œâ”€â”€ orchestration/
â”‚   â”œâ”€â”€ task-scheduler-configs/      # Windows Task Scheduler XMLs
â”‚   â””â”€â”€ batch-scripts/               # Orchestration batch files
â”œâ”€â”€ data-quality/
â”‚   â”œâ”€â”€ validation-rules/            # Business rule definitions
â”‚   â””â”€â”€ anomaly-detection/           # 3-tier detection system
â””â”€â”€ docs/
    â”œâ”€â”€ performance-benchmarks.md    # Speed comparisons
    â”œâ”€â”€ reliability-metrics.md       # SLA tracking
    â””â”€â”€ deployment-guide.md          # Production setup
```

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.10+
python --version

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows
```

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Verify installation
python -c "import polars; import duckdb; print('Ready!')"
```

### Run a Pipeline

```bash
# Execute a single pipeline
python -m etl_framework.run --pipeline inventory_daily

# Execute all pipelines for a source
python -m etl_framework.run --source erp

# Dry run (no writes)
python -m etl_framework.run --pipeline inventory_daily --dry-run
```

---

## ğŸ’¡ Key Design Decisions

### Why Polars over Pandas?

| Operation | Pandas | Polars | Speedup |
|-----------|--------|--------|---------|
| CSV Read (1GB) | 45s | 8s | **5.6x** |
| GroupBy Agg | 12s | 1.2s | **10x** |
| Join (2 tables) | 8s | 0.9s | **8.9x** |
| Memory Usage | 4GB | 1.1GB | **3.6x** |

### Why Parquet with Hive Partitioning?

- **Columnar storage** â†’ Only read columns you need
- **Compression** â†’ 80% smaller than CSV
- **Partition pruning** â†’ Query only relevant date ranges
- **Schema evolution** â†’ Add columns without breaking readers

### Why Configuration-Driven Pipelines?

```yaml
# New data source? Just add config:
pipelines:
  - name: new_source_daily
    source:
      type: database
      connection: ${NEW_SOURCE_CONN}
      query: "SELECT * FROM table"
    destination:
      layer: bronze
      partition_by: [extract_date]
    schedule: "0 6 * * *"
```

---

## ğŸ“Š Data Model

### Fact Tables (Gold Layer)

| Table | Grain | Key Metrics |
|-------|-------|-------------|
| `fact_sales` | Transaction | Revenue, Quantity, Discount |
| `fact_inventory` | SKU Ã— Location Ã— Day | On-hand, In-transit, Reserved |
| `fact_orders` | Order Line | Order qty, Fulfilled qty, Lead time |
| `fact_replenishment` | SKU Ã— Day | Reorder point, Safety stock, EOQ |
| `fact_forecast` | SKU Ã— Week | Predicted demand, Confidence |

### Dimension Tables

| Table | Attributes |
|-------|------------|
| `dim_product` | SKU, Category, Brand, Supplier |
| `dim_location` | Store, Warehouse, Region, Country |
| `dim_time` | Date, Week, Month, Quarter, Year |
| `dim_supplier` | Supplier, Lead time, MOQ |
| `dim_customer` | Customer segment, Loyalty tier |
| `dim_channel` | Online, Retail, Wholesale |

---

## ğŸ”’ Data Quality

### 3-Tier Anomaly Detection

```
Tier 1: VALIDATION
â”œâ”€â”€ Schema conformance
â”œâ”€â”€ Required fields
â”œâ”€â”€ Data type enforcement
â””â”€â”€ Referential integrity

Tier 2: OUTLIER DETECTION
â”œâ”€â”€ Statistical bounds (IQR, Z-score)
â”œâ”€â”€ Historical range checks
â””â”€â”€ Velocity checks (rate of change)

Tier 3: BUSINESS RULES
â”œâ”€â”€ Domain-specific validations
â”œâ”€â”€ Cross-field consistency
â””â”€â”€ Temporal logic checks
```

### Results

- **500+ dimension anomalies** identified and fixed
- **70% reduction** in data-related incidents
- **Automated alerting** for critical issues

---

## ğŸ“ˆ Performance Benchmarks

| Pipeline | Records/Day | Avg Runtime | P95 Runtime |
|----------|-------------|-------------|-------------|
| Inventory Sync | 2.5M | 12 min | 18 min |
| Sales Transactions | 500K | 8 min | 12 min |
| Replenishment Calc | 10K SKUs | 25 min | 35 min |
| Forecast Generation | 10K SKUs | 45 min | 60 min |

---

## ğŸ› ï¸ Tech Stack

| Category | Technology |
|----------|------------|
| **Language** | Python 3.10+ |
| **Data Processing** | Polars, DuckDB |
| **Storage Format** | Parquet (Hive-partitioned) |
| **Databases** | SQL Server, Oracle |
| **Automation** | Selenium, PyAutoGUI |
| **Visualization** | Streamlit, Power BI |
| **Orchestration** | Windows Task Scheduler |
| **Version Control** | Git |

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

---

## ğŸ‘¤ Author

**Godson Kurishinkal**  
Senior Data Engineer | Dubai, UAE

- GitHub: [@GodsonKurishinkal](https://github.com/GodsonKurishinkal)
- LinkedIn: [godsonkurishinkal](https://linkedin.com/in/godsonkurishinkal)
- Portfolio: [godsonkurishinkal.github.io](https://godsonkurishinkal.github.io/data-engineering-portfolio)
